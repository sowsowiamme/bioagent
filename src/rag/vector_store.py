# target_discovery/vector_store.py
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import pickle
import os
from typing import List, Dict, Tuple

print(faiss.__version__)

class TargetVectorStore:
    """åŸºäºFAISSçš„å‘é‡å­˜å‚¨ï¼Œç”¨äºè¯­ä¹‰æœç´¢"""
    
    def __init__(self, model_name="BAAI/bge-small-en-v1.5"):
        print(f"â³ åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.index = None 
        self.documents = []
        self.embeddings = []
    
    def add_documents(self, documents: List[Dict]):
        """å°†æ–‡æ¡£æ·»åŠ è‡³å‘é‡åº“"""
        texts = []
        for doc in documents:
            text = f"{doc.get('title', '')} {doc.get('abstract', '')}"
            texts.append(text)
            self.documents.append(doc)

    def add_documents(self, documents: List[Dict]):
        """å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡åº“"""
        texts = []
        for doc in documents:
            # merge the title of the doc, and the abstract of the doc
            text = f"{doc.get('title', '')} {doc.get('abstract', '')}"
            texts.append(text)
            self.documents.append(doc)
        
        # ç”ŸæˆåµŒå…¥
        print(f"â³ ä¸º {len(texts)} ç¯‡æ–‡çŒ®ç”ŸæˆåµŒå…¥...")
        batch_size = 32
        all_embeddings = []        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            embeddings = self.model.encode(batch, normalize_embeddings=True)
            all_embeddings.append(embeddings)
            
        self.embeddings = np.vstack(all_embeddings)
        
        # æ„å»ºFAISSç´¢å¼•
        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç›¸ä¼¼åº¦ï¼ˆå½’ä¸€åŒ–åç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.index.add(self.embeddings.astype(np.float32))
        
        print(f"âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼Œå…± {len(self.documents)} ç¯‡æ–‡çŒ®")
    

    def search(self, query: str, top_k: int=10):
        if self.index is None or len(self.documents) ==0:
            return []
        query_embedding = self.model.encode([query], normalize_embeddings=True)

        scores, indices = self.index.search(query_embedding.astype(np.float32), top_k)
        
    def search(self, query: str, top_k: int = 10) -> List[Tuple[Dict, float]]:
        """è¯­ä¹‰æœç´¢ç›¸å…³æ–‡çŒ®"""
        if self.index is None or len(self.documents) == 0:
            return []
        
        # æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = self.model.encode([query], normalize_embeddings=True)
        
        # æœç´¢
        scores, indices = self.index.search(query_embedding.astype(np.float32), top_k)
        
        # return results
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx < len(self.documents):
                results.append((self.documents[idx], float(score)))
                
        return results
    
    
    def save(self, path: str):
        """ä¿å­˜å‘é‡åº“åˆ°ç£ç›˜"""
        os.makedirs(path, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        if self.index is not None:
            faiss.write_index(self.index, f"{path}/index.faiss")
        
        # ä¿å­˜æ–‡æ¡£å’ŒåµŒå…¥
        with open(f"{path}/documents.pkl", "wb") as f:
            pickle.dump(self.documents, f)
        
        if len(self.embeddings) > 0:
            np.save(f"{path}/embeddings.npy", self.embeddings)
            
        print(f"âœ… å‘é‡åº“å·²ä¿å­˜åˆ° {path}")
    
    def load(self, path: str):
        """ä»ç£ç›˜åŠ è½½å‘é‡åº“"""
        # åŠ è½½FAISSç´¢å¼•
        index_path = f"{path}/index.faiss"
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
        
        # åŠ è½½æ–‡æ¡£
        with open(f"{path}/documents.pkl", "rb") as f:
            self.documents = pickle.load(f)
        
        # åŠ è½½åµŒå…¥
        embeddings_path = f"{path}/embeddings.npy"
        if os.path.exists(embeddings_path):
            self.embeddings = np.load(embeddings_path)
            
        print(f"âœ… ä» {path} åŠ è½½å‘é‡åº“å®Œæˆï¼Œå…± {len(self.documents)} ç¯‡æ–‡çŒ®")


# test_vector_store.py

def main():
    print("ğŸš€ æµ‹è¯• FAISS å‘é‡å­˜å‚¨...")
    
    # åˆå§‹åŒ–å‘é‡åº“
    store = TargetVectorStore(model_name="BAAI/bge-small-en-v1.5")
    
    # æ¨¡æ‹Ÿä¸€äº›æ–‡çŒ®æ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­è¿™äº›åº”æ¥è‡ª PubMedï¼‰
    mock_docs = [
        {
            "title": "PD-1 blockade in breast cancer",
            "abstract": "PD-1 is a key immune checkpoint. Anti-PD-1 antibodies have shown efficacy in triple-negative breast cancer.",
            "year": "2023",
            "pmid": "12345678"
        },
        {
            "title": "HER2-targeted antibody-drug conjugates",
            "abstract": "Trastuzumab deruxtecan is a novel HER2-directed antibody-drug conjugate with promising activity in breast cancer.",
            "year": "2022",
            "pmid": "23456789"
        },
        {
            "title": "CTLA-4 immunotherapy in solid tumors",
            "abstract": "CTLA-4 blockade with ipilimumab has been explored in melanoma and other cancers, including breast cancer.",
            "year": "2021",
            "pmid": "34567890"
        }
    ]
    
    print(f"\nğŸ“š æ·»åŠ  {len(mock_docs)} ç¯‡æ¨¡æ‹Ÿæ–‡çŒ®åˆ°å‘é‡åº“...")
    store.add_documents(mock_docs)
    
    # æµ‹è¯•è¯­ä¹‰æœç´¢
    queries = [
        "immune checkpoint inhibitors in breast cancer",
        "HER2 therapy",
        "CTLA-4 antibodies"
    ]
    
    for query in queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        results = store.search(query, top_k=2)
        
        if results:
            for doc, score in results:
                print(f"  - {doc['title']} (ç›¸ä¼¼åº¦: {score:.4f})")
        else:
            print("  æ— ç»“æœ")
    
    # æµ‹è¯•ä¿å­˜ä¸åŠ è½½
    save_path = "./test_vector_store"
    print(f"\nğŸ’¾ ä¿å­˜å‘é‡åº“åˆ° {save_path}")
    store.save(save_path)
    
    print("ğŸ“‚ é‡æ–°åŠ è½½å‘é‡åº“")
    new_store = TargetVectorStore()
    new_store.load(save_path)
    
    # éªŒè¯åŠ è½½åçš„æœç´¢åŠŸèƒ½
    print("\nğŸ” é‡æ–°åŠ è½½åæœç´¢ 'PD-1'")
    results = new_store.search("PD-1", top_k=2)
    for doc, score in results:
        print(f"  - {doc['title']} (ç›¸ä¼¼åº¦: {score:.4f})")
    
    print("\nâœ… FAISS å‘é‡å­˜å‚¨æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()