from langchain_community.document_loaders import PubMedLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from modelscope import snapshot_download
from sentence_transformers import SentenceTransformer
import torch
import os
import time
import sys 


class TargetDiscoveryRAG:
    def __init__(self, cache_dir= "./data/rag_cache"):
        # model_path = './all-MiniLM-L6-v2'
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        # self.embeddings = HuggingFaceEmbeddings(
            # model_name = "sentence-transformers/all-MiniLM-L6-v2",
            # model_kwargs = { "device": "mps" if torch.backends.mps.is_available() else "cpu"}
        #     model_name = model_path, 
        #     model_kwargs={'device': 'cpu'},  # æ ¹æ®éœ€è¦æ”¹ä¸º 'cuda'
        #     encode_kwargs={
        #         'normalize_embeddings': True,
        #         'show_progress_bar': False
        #     }
            
        # )
                # âœ… å…³é”®2ï¼šä»ModelScopeä¸‹è½½æ¨¡å‹ï¼ˆå›½å†…ç›´è¿ï¼Œ100%æˆåŠŸï¼‰
        model_dir = snapshot_download(
            'AI-ModelScope/all-MiniLM-L6-v2',  # ModelScopeé•œåƒ
            cache_dir=os.path.join(cache_dir, "models")
        )
        
        # # âœ… å…³é”®3ï¼šä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼ˆæ— ç½‘ç»œè¯·æ±‚ï¼‰
        self.embeddings = SentenceTransformer(model_dir)
        self.vectorstore = None


    def build_knowledge_base(self,diseases= ["lung cancer", "breaset_cancer"]):
        """knowledge database for diseases"""
        cache_path = os.path.join(self.cache_dir, "target_kb")
        if os.path.exists(cache_path):
            print("ä»ç¼“å­˜ä¸­åŠ è½½çŸ¥è¯†åº“")
            self.vectorstore = FAISS.load_local(cache_path, self.embeddings, allow_dangerous_deserialization=True)
            return         
        print("â³ é¦–æ¬¡æ„å»ºçŸ¥è¯†åº“ï¼ˆçº¦10åˆ†é’Ÿï¼‰...")
        docs = []
        for disease in diseases:
            # ä»PubMedåŠ è½½é¶ç‚¹ç›¸å…³æ–‡çŒ®ï¼ˆæ¯ç–¾ç—…5ç¯‡ï¼‰
            loader = PubMedLoader(f"{disease} target therapy")
            docs.extend(loader.load())
        
        # æ„å»ºå‘é‡åº“
        self.vectorstore = FAISS.from_documents(docs, self.embeddings)
        self.vectorstore.save_local(cache_path)
        print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼å…±{len(docs)}ç¯‡æ–‡çŒ®")
    

    def discover_targets(self, disease:str, top_k:int=3) -> dict:
        """è¾“å…¥ç–¾ç—…ï¼Œè¾“å‡ºé¶ç‚¹å‡è®¾ï¼Œ+æ–‡çŒ®è¯æ®"""
        if self.vectorstore is None:
            self.build_knowledge_base()
        # æ£€ç´¢ç›¸å…³æ–‡çŒ®
        query = f"therapeutic targets for {disease} treatment mechanism"
        results = self.vectorstore.similarity_search(query, k = top_k)
        targets = []
        for i, doc in enumerate(results):
            content = doc.page_content.lower()
            candidate_targets = []
            known_targets = ["pd-1", "pd-l1", "ctla-4", "her2", "egfr", "vegf", "parp", "brca"]
            for target in known_targets:
                if target in content:
                    candidate_targets.append(target.upper())

            if candidate_targets:
                targets.append({
                    "target": ", ".join(set(candidate_targets)),
                    "evidence": doc.page_content[:300] + "...",
                    "source": doc.metadata.get("uid", "PubMed"),
                    "relevance_score": 1.0 - i*0.2  # æ¨¡æ‹Ÿç›¸å…³æ€§
                })
        
        return {
            "disease": disease,
            "targets": targets[:3],  # è¿”å›Top 3é¶ç‚¹
            "query_time": "0.8s"  # æ¨¡æ‹Ÿå“åº”æ—¶é—´
        }



if __name__ == "__main__":
    # start = time.time()
    # try:
    #     rag = TargetDiscoveryRAG(cache_dir="./data/rag_cache")
    #     print(f"âœ… RAGåˆå§‹åŒ–æˆåŠŸ ({time.time()-start:.2f}s)")
    # except Exception as e:
    #     print(f"âŒ RAGåˆå§‹åŒ–å¤±è´¥: {e}")
    #     import traceback
    #     traceback.print_exc()
    #     sys.exit(1)
    # Step 2: æ„å»ºçŸ¥è¯†åº“ï¼ˆçœŸå®PubMedæ£€ç´¢ï¼‰
    print("\n[2/3] ğŸ” ä»PubMedæ£€ç´¢çœŸå®æ–‡çŒ®ï¼ˆlung cancer targetsï¼‰...")
    print("    â³ é¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½æ–‡çŒ®ï¼ˆçº¦1-3åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
    start = time.time()
    try:
        # å…³é”®ï¼šå‡å°‘max_resultsé¿å…è¶…æ—¶ï¼Œèšç„¦é«˜è´¨é‡æ–‡çŒ®
        loader = PubMedLoader("non-small cell lung cancer PD-1 EGFR therapy")
        docs = loader.load()
        
        print(f"âœ… PubMedæ£€ç´¢æˆåŠŸï¼è·å– {len(docs)} ç¯‡çœŸå®æ–‡çŒ® ({time.time()-start:.2f}s)")
        for i, doc in enumerate(docs):
            pmid = doc.metadata.get('uid', 'N/A')
            title_preview = doc.page_content[:60].split('. ')[0] + "..."  # æˆªæ–­åˆ°ç¬¬ä¸€ä¸ªå¥å·
        
            print(f"   ğŸ“„ [{i+1}] PMID:{pmid}")
            print(f"      æ‘˜è¦é¢„è§ˆ: {title_preview}")
    except Exception as e:
        print(e)